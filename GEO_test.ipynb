{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3d78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from ftplib import FTP\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import tempfile\n",
    "import os\n",
    "import tarfile\n",
    "import scanpy as sc\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b607530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Claude API key from local text file\n",
    "# check if we're on MacOS or Windows and read appropriate file\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    with open(\"C:/Users/David/.claude_api.txt\") as f:\n",
    "        claude_key = f.read().strip()\n",
    "else:\n",
    "    with open(\"/Users/tatarakis/.api-keys/tatarakis-test-key.txt\") as f:\n",
    "        claude_key = f.read().strip()\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = claude_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16957258",
   "metadata": {},
   "source": [
    "# Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65aead36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='f8b6e9bc-f6a9-434b-8f8e-82c72a7c373e'),\n",
       "  AIMessage(content=[{'id': 'toolu_018siAXHhZCV5dUKSpKe6Zqp', 'input': {'city': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01GgAKCXgECBaRLfMPNt3LFC', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 569, 'output_tokens': 54, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019b8ac9-08e6-7200-88b7-536301492fb4-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'toolu_018siAXHhZCV5dUKSpKe6Zqp', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 569, 'output_tokens': 54, 'total_tokens': 623, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='00e63fec-a6f1-4833-b15d-1119b03d7905', tool_call_id='toolu_018siAXHhZCV5dUKSpKe6Zqp'),\n",
       "  AIMessage(content='The weather in San Francisco is sunny! ☀️', additional_kwargs={}, response_metadata={'id': 'msg_01Y2nqyoDp6YgLSbog3nPcBN', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 643, 'output_tokens': 15, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019b8ac9-163e-77c1-9cdb-f7e36866c902-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 643, 'output_tokens': 15, 'total_tokens': 658, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca23722",
   "metadata": {},
   "source": [
    "# State Structure\n",
    "\n",
    "Each node/task the agent performs will need to keep track of inputs and outputs. The graph state class allows us to provide all of the possible inputs and outputs the agent will need, such that all nodes can see it and update it as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cb253",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "We need to define the set of tools the agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fe0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_ftp_path(accession: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the FTP directory for a GEO accession (GSE or GSM).\n",
    "    \"\"\"\n",
    "    prefix = accession[:3]     # GSE or GSM\n",
    "    number = accession[3:]\n",
    "    chunk = prefix + number[:-3] + \"nnn\"\n",
    "\n",
    "    # the ftp site stores series and samples in directories named by the accession number with the last three digits replaced by 'nnn'\n",
    "    if prefix == \"GSE\":\n",
    "        return f\"/geo/series/{chunk}/{accession}/suppl/\"\n",
    "    elif prefix == \"GSM\":\n",
    "        return f\"/geo/samples/{chunk}/{accession}/suppl/\"\n",
    "    else:\n",
    "        raise ValueError(\"Only GSE or GSM supported\")\n",
    "\n",
    "def list_geo_files(accession: str):\n",
    "\n",
    "    ftp = FTP(\"ftp.ncbi.nlm.nih.gov\")\n",
    "    ftp.login()\n",
    "\n",
    "    path = get_geo_ftp_path(accession)\n",
    "    try:\n",
    "        ftp.cwd(path)\n",
    "    except:\n",
    "        try:\n",
    "            path = re.sub(r\"suppl/$\", \"\", path)\n",
    "            ftp.cwd(path)\n",
    "            warnings.warn(f\"No supplementary files for: {accession}\")\n",
    "        except:\n",
    "            raise FileNotFoundError(f\"Could not find FTP path: {path}\")\n",
    "\n",
    "    files = ftp.nlst()\n",
    "    ftp.quit()\n",
    "    return files\n",
    "\n",
    "def download_geo_supp_file(accession: str, file_name:str, output_dir: str):\n",
    "    ftp = FTP(\"ftp.ncbi.nlm.nih.gov\")\n",
    "    ftp.login()\n",
    "    \n",
    "    path = get_geo_ftp_path(accession)\n",
    "    try:\n",
    "        ftp.cwd(path)\n",
    "    except:\n",
    "        try:\n",
    "            path = re.sub(r\"suppl/$\", \"\", path)\n",
    "            ftp.cwd(path)\n",
    "            warnings.warn(f\"No supplementary files for: {accession}\")\n",
    "        except:\n",
    "            raise FileNotFoundError(f\"Could not find FTP path: {path}\")\n",
    "\n",
    "    local_file_path = os.path.join(output_dir, file_name)\n",
    "    with open(local_file_path, \"wb\") as f:\n",
    "        try:\n",
    "            ftp.retrbinary(f\"RETR {file_name}\", f.write)\n",
    "        except Exception as e:\n",
    "            ftp.quit()\n",
    "            raise e\n",
    "    ftp.quit()\n",
    "    return local_file_path\n",
    "\n",
    "def list_tar_contents(file_name: str):\n",
    "    contents = []\n",
    "    with tarfile.open(file_name, \"r:*\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            print(member.name)\n",
    "            contents.append(member.name)\n",
    "    return contents\n",
    "\n",
    "# adding a tool to batch view contents of tar files\n",
    "def batch_list_tar_contents(directory: str):\n",
    "    tar_contents = {}\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar\") or file.endswith(\".tar.gz\") or file.endswith(\".tgz\"):\n",
    "            tar_file_path = os.path.join(directory, file)\n",
    "            contents = list_tar_contents(tar_file_path)\n",
    "            tar_contents[file] = contents\n",
    "    return tar_contents\n",
    "\n",
    "def unpack_tar_file(tar_file_path: str, output_dir: str):\n",
    "    with tarfile.open(tar_file_path, \"r\") as tar:\n",
    "        tar.extractall(path=output_dir)\n",
    "        return [member.name for member in tar.getmembers()]\n",
    "\n",
    "# adding a tool for batch unpacking tar files\n",
    "def batch_unpack_tar_files(directory: str):\n",
    "    unpacked_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".tar\") or file.endswith(\".tar.gz\") or file.endswith(\".tgz\"):\n",
    "            tar_file_path = os.path.join(directory, file)\n",
    "            unpacked = unpack_tar_file(tar_file_path, directory)\n",
    "            unpacked_files.extend(unpacked)\n",
    "    return unpacked_files\n",
    "\n",
    "# adding a tool to let the agent look at the directory\n",
    "def list_directory(directory: str) -> list:\n",
    "    \"\"\"List all files and folders in a directory.\"\"\"\n",
    "    return os.listdir(directory)\n",
    "\n",
    "def build_anndata(counts_directory: str, sample_name: str, outdir: str):\n",
    "    adata = sc.read_10x_mtx(counts_directory)\n",
    "    \n",
    "    # add sample name to obs and store anndata in dictionary\n",
    "    adata.obs[\"sample_name\"] = sample_name\n",
    "    # adatas[sample_name] = adata\n",
    "    \n",
    "    # make a subdirectory to store anndata files\n",
    "    adata_dir = os.path.join(outdir, \"adatas\")\n",
    "    os.makedirs(adata_dir, exist_ok=True)\n",
    "    adata.write_h5ad(os.path.join(adata_dir, f\"{sample_name}.h5ad\"))\n",
    "\n",
    "    # check that the file was actually saved\n",
    "    saved_file_path = os.path.join(adata_dir, f\"{sample_name}.h5ad\")\n",
    "    if os.path.exists(saved_file_path):\n",
    "        print(f\"Anndata object successfully saved at: {saved_file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Failed to save anndata object at: {saved_file_path}\")\n",
    "    \n",
    "    return saved_file_path\n",
    "\n",
    "# Rename files according to 10x Genomics conventions\n",
    "def rename_geo_files(directory: str):\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    matrix = None\n",
    "    features = None\n",
    "    barcodes = None\n",
    "\n",
    "    for f in files:\n",
    "        n = f.lower()\n",
    "\n",
    "        # matrix\n",
    "        if \"mtx\" in n:\n",
    "            matrix = f\n",
    "            continue\n",
    "\n",
    "        # features (genes)\n",
    "        if any(x in n for x in [\"gene\", \"feature\", \"symbol\"]):\n",
    "            features = f\n",
    "            continue\n",
    "\n",
    "        # barcodes (cells)\n",
    "        if any(x in n for x in [\"barcode\", \"cell\"]):\n",
    "            barcodes = f\n",
    "            continue\n",
    "\n",
    "    # Safety check\n",
    "    if not (matrix and features and barcodes):\n",
    "        raise ValueError(\n",
    "            f\"Could not find all required files in {directory}. \"\n",
    "            f\"Found matrix={matrix}, features={features}, barcodes={barcodes}\"\n",
    "        )\n",
    "\n",
    "    rename_map = {\n",
    "        matrix: \"matrix.mtx\",\n",
    "        features: \"features.tsv\",\n",
    "        barcodes: \"barcodes.tsv\"\n",
    "    }\n",
    "\n",
    "    # check if files are gzipped and add appropriate extension to the new name\n",
    "    for key, value in list(rename_map.items()):\n",
    "        if key.endswith(\".gz\"):\n",
    "            rename_map[key] = value + \".gz\"\n",
    "\n",
    "    # apply the new names by using a bash mv command\n",
    "    for old, new in rename_map.items():\n",
    "        src = os.path.join(directory, old)\n",
    "        dst = os.path.join(directory, new)\n",
    "        shutil.move(src, dst)\n",
    "        print(f\"Renamed {src} → {dst}\")\n",
    "\n",
    "    return directory, rename_map\n",
    "\n",
    "# structure 10x directory\n",
    "def structure_10x_directory(directory):\n",
    "    # create a new directory for the 10x files\n",
    "    counts_directory = os.path.join(directory, \"10x_counts\")\n",
    "    os.makedirs(counts_directory, exist_ok=True)\n",
    "\n",
    "    # move the relevant files to the new directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file in [\"matrix.mtx\", \"matrix.mtx.gz\", \"features.tsv\", \"features.tsv.gz\", \"barcodes.tsv\", \"barcodes.tsv.gz\"]:\n",
    "            src = os.path.join(directory, file)\n",
    "            dst = os.path.join(counts_directory, file)\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    return counts_directory\n",
    "\n",
    "# convert csv files to tsv files\n",
    "def convert_csv_to_tsv(file_path):\n",
    "        \n",
    "    # handle uncompressed csvs\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        tsv_file_path = file_path[:-4] + \".tsv\" # change suffix\n",
    "        with open(file_path, \"r\") as csv_file, open(tsv_file_path, \"w\") as tsv_file:\n",
    "            for line in csv_file:\n",
    "                tsv_file.write(line.replace(\",\", \"\\t\"))\n",
    "\n",
    "    # handle gzipped csvs\n",
    "    if file_path.endswith(\".csv.gz\"):\n",
    "        tsv_file_path = file_path[:-7] + \".tsv.gz\" # change suffix\n",
    "        with gzip.open(file_path, \"rt\") as csv_file, gzip.open(tsv_file_path, \"wt\") as tsv_file:\n",
    "            for line in csv_file:\n",
    "                tsv_file.write(line.replace(\",\", \"\\t\"))\n",
    "\n",
    "    if not file_path.endswith(\".csv.gz\") and not file_path.endswith(\".csv\"):\n",
    "        tsv_file_path = file_path  # no conversion needed\n",
    "    return tsv_file_path\n",
    "\n",
    "# a simple function to get the dimensions of the counts matrix to help with reformatting input files\n",
    "def get_matrix_dimensions(matrix_file_path: str) -> tuple:\n",
    "    # get dimensions of sparse matrix\n",
    "    opener = gzip.open if matrix_file_path.endswith(\".gz\") else open\n",
    "\n",
    "    with opener(matrix_file_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # skip comments and header\n",
    "            if line.startswith(\"%\") or line.startswith(\"%%\"):\n",
    "                continue\n",
    "\n",
    "            # first non-comment line should be: rows cols nnz\n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                rows, cols, nnz = map(int, parts)\n",
    "                return rows, cols, nnz\n",
    "    raise ValueError(\"Could not determine matrix dimensions from file.\")\n",
    "\n",
    "# check the formatting of the features file and reformat if necessary.\n",
    "# likely problems are only a single column or a header row when there shouldn't be one.\n",
    "def format_features_file(features_file_path: str, matrix_dimensions: tuple):\n",
    "\n",
    "    # if file contains only one column, add a second column identical to the first with name gene_id\n",
    "    if features_file_path.endswith(\".gz\"):\n",
    "        with gzip.open(features_file_path, \"rt\") as f:\n",
    "            lines = f.readlines()\n",
    "    else:\n",
    "        with open(features_file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "    \n",
    "    # check if there is exactly one more row than the number of rows in the matrix (indicating a header)\n",
    "    nrows = len(lines)\n",
    "    if nrows == matrix_dimensions[0] + 1:\n",
    "        header = True\n",
    "    else:\n",
    "        header = False\n",
    "    \n",
    "    # if there's a header, parse accordingly\n",
    "    if header:\n",
    "        header = lines[0].strip().split(\"\\t\")\n",
    "        rows = [line.strip().split(\"\\t\") for line in lines[1:]]\n",
    "    else:\n",
    "        rows = [line.strip().split(\"\\t\") for line in lines]\n",
    "\n",
    "    # read lines as a pandas dataframe to check number of columns\n",
    "    features_df = pd.DataFrame(rows)\n",
    "    # if only one column, duplicate it as column 2 and add dummy column 3\n",
    "    if features_df.shape[1] == 1:\n",
    "        features_df[1] = features_df.iloc[:, 0]\n",
    "        features_df[2] = \"gene\"\n",
    "    elif features_df.shape[1] == 2:\n",
    "        features_df[2] = \"gene\"\n",
    "    # keep only first three columns if more are present\n",
    "    features_df = features_df.iloc[:, :3]\n",
    "    \n",
    "    # write the reformatted features file back to disk\n",
    "    if features_file_path.endswith(\".gz\"):\n",
    "        with gzip.open(features_file_path, \"wt\") as f:\n",
    "            features_df.to_csv(f, sep=\"\\t\", index=False, header = False)\n",
    "    else:\n",
    "        with open(features_file_path, \"w\") as f:\n",
    "                features_df.to_csv(f, sep=\"\\t\", index=False, header = False)\n",
    "\n",
    "    return features_file_path\n",
    "\n",
    "def format_barcodes_file(barcodes_file_path: str, matrix_dimensions: tuple):\n",
    "\n",
    "    # read barcodes file\n",
    "    if barcodes_file_path.endswith(\".gz\"):\n",
    "        with gzip.open(barcodes_file_path, \"rt\") as f:\n",
    "            lines = f.readlines()\n",
    "    else:\n",
    "        with open(barcodes_file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "    \n",
    "    # check if there is exactly one more row than the number of columns in the matrix (indicating a header)\n",
    "    nrows = len(lines)\n",
    "    if nrows == matrix_dimensions[1] + 1:\n",
    "        header = True\n",
    "    else:\n",
    "        header = False\n",
    "    \n",
    "    # if there's a header, parse accordingly\n",
    "    if header:\n",
    "        header = lines[0].strip().split(\"\\t\")\n",
    "        rows = [line.strip().split(\"\\t\") for line in lines[1:]]\n",
    "    else:\n",
    "        rows = [line.strip().split(\"\\t\") for line in lines]\n",
    "        \n",
    "\n",
    "    # read lines as a pandas dataframe to check number of columns\n",
    "    barcodes_df = pd.DataFrame(rows)\n",
    "    # keep only first column if more are present\n",
    "    barcodes_df = barcodes_df.iloc[:, :1]\n",
    "    \n",
    "     # strip any suffix\n",
    "    tenx_pattern = r\"([ACGTN]{16,20}-\\d+)\"\n",
    "    barcodes_df[barcodes_df.columns[0]] = barcodes_df[barcodes_df.columns[0]].str.extract(tenx_pattern)\n",
    "\n",
    "    # write the reformatted barcodes file back to disk\n",
    "    if barcodes_file_path.endswith(\".gz\"):\n",
    "        with gzip.open(barcodes_file_path, \"wt\") as f:\n",
    "            barcodes_df.to_csv(f, sep=\"\\t\", index=False, header = False)\n",
    "    else:\n",
    "        with open(barcodes_file_path, \"w\") as f:\n",
    "                barcodes_df.to_csv(f, sep=\"\\t\", index=False, header = False)\n",
    "\n",
    "    return barcodes_file_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b062b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    query: str\n",
    "    geo_accession: str\n",
    "    files_url: list[str]\n",
    "    download_path: str\n",
    "    counts_path: str\n",
    "    anndata: object | None\n",
    "    anndata_path: str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
